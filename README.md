# Events Service

[![CI/CD](https://github.com/Verteletsky/events-service/actions/workflows/ci.yml/badge.svg)](https://github.com/Verteletsky/events-service/actions/workflows/ci.yml)

Сервис для управления жизненным циклом событий.

## Описание

Сервис предоставляет API для управления событиями в системе. Он позволяет:

- Создавать новые события
- Получать список событий
- Запускать события
- Завершать события

## Технологии

- Go 1.23.4
- MongoDB
- Docker
- GitHub Actions (CI/CD)

## Требования

- Docker
- Docker Compose
- Go 1.23.4
- MongoDB 6.0 или выше

## Переменные окружения

Сервис поддерживает следующие переменные окружения:

| Переменная         | Описание                      | Значение по умолчанию       |
|--------------------|-------------------------------|-----------------------------|
| `MONGODB_URI`      | URI для подключения к MongoDB | `mongodb://localhost:27017` |
| `MONGODB_DATABASE` | Имя базы данных               | `events`                    |
| `SERVER_PORT`      | Порт HTTP сервера             | `8080`                      |
| `LOG_LEVEL`        | Уровень логирования           | `info`                      |

## Особенности

- Использование MongoDB для хранения данных
- REST API для управления событиями
- Логирование с помощью zap (Uber)
- Graceful shutdown
- Docker контейнеризация
- Валидация входных данных
- Тесты производительности
- Оптимистичная блокировка для предотвращения конфликтов

## Запуск

1. Клонируйте репозиторий:

```bash
git clone https://github.com/Verteletsky/events-service.git
cd events-service
```

2. Создайте файл `.env` (опционально):

```bash
MONGODB_URI=mongodb://localhost:27017
MONGODB_DATABASE=events
SERVER_PORT=8080
LOG_LEVEL=info
```

3. Запустите сервис с помощью Docker Compose:

```bash
docker-compose up -d
```

Сервис будет доступен по адресу: http://localhost:8080

## API Endpoints

### GET /v1

Получение списка событий

Параметры запроса:

- `offset` (опционально) - смещение (по умолчанию 0)
- `limit` (опционально) - количество событий (максимум 100, по умолчанию 100)
- `type` (опционально) - фильтр по типу события

### POST /v1/start

Создание нового события

Тело запроса:

```json
{
  "type": "string"
  // тип события (только строчные буквы и цифры)
}
```

### POST /v1/finish

Завершение существующего события

Тело запроса:

```json
{
  "type": "string"
  // тип события (только строчные буквы и цифры)
}
```

## Валидация

Сервис выполняет следующие проверки:

- Тип события должен соответствовать регулярному выражению `^[a-z0-9]+$`
- Параметр `limit` не может быть больше 100
- Не может быть более одного незавершенного события одного типа

## Тесты

### Unit-тесты

```bash
go test ./... -v
```

### Тесты производительности

Для запуска тестов производительности:

```bash
node load-test.js
```

## Результаты нагрузочного тестирования

### Тестирование с разным количеством воркеров и размером буфера

| Метрика | 15 воркеров (буфер 1000) | 30 воркеров (буфер 1000) | 50 воркеров (буфер 1000) | 50 воркеров (буфер 10000) | 50 воркеров (буфер 5000) |
|---------|--------------------------|--------------------------|--------------------------|---------------------------|--------------------------|
| RPS | 3,815 | 3,801 | 3,932 | 3,877 | 3,988 |
| Процент ошибок | 22.44% | 16.34% | 12.34% | 11.77% | 10.56% |
| p95 latency | 73.15ms | 71.32ms | 63.65ms | 66.21ms | 58.06ms |

**Выводы**:
- Оптимальная конфигурация: 50 воркеров с буфером 5000
- Максимальная производительность: ~3,988 RPS
- Минимальное время отклика: 58.06ms (p95)
- Минимальный процент ошибок: 10.56%

P.S. Стоит пересмотреть load-test.js

1. Добавить rate limiter для защиты от перегрузки
2. Оптимизировать работу с MongoDB для дальнейшего снижения ошибок
3. Добавить мониторинг размера очереди и метрик производительности

## Разработка

Для локальной разработки:

1. Установите зависимости:

```bash
go mod download
```

2. Запустите MongoDB:

```bash
docker-compose up -d mongodb
```

3. Запустите сервис:

```bash
go run cmd/main.go
```

## Логирование

Сервис использует zap логгер от Uber для логирования. Логи включают:

- Информацию о запуске и остановке сервиса
- Ошибки при работе с базой данных
- Информацию о создании и завершении событий
- Предупреждения при попытке завершить несуществующее событие

## Оптимизация Docker образа

В процессе разработки были проведены оптимизации Docker образа, что позволило значительно уменьшить его размер:

### Результаты оптимизации

- Исходный размер: 38.7MB
- После первой оптимизации: 16.6MB (уменьшение на ~57%)
- После полной оптимизации: 6.4MB (уменьшение на ~83% от исходного размера)

### Примененные оптимизации

1. **Многоступенчатая сборка**
    - Использование `scratch` вместо `alpine` в финальном образе
    - Минимальное количество слоев

2. **Оптимизация сборки Go**
    - Статическая линковка (`-extldflags=-static`)
    - Удаление путей из бинарника (`-trimpath`)
    - Отключение отладочной информации (`-w -s`)

3. **Сжатие бинарника**
    - Использование `upx` с алгоритмом LZMA
    - Оптимальный уровень сжатия (`--best`)

4. **Исключение ненужных файлов**
    - Использование `.dockerignore`
    - Копирование только необходимых файлов
    - Исключение тестов, документации и временных файлов

### Преимущества оптимизированного образа

- Минимальный размер (6.4MB)
- Быстрая загрузка и развертывание
- Меньше уязвимостей (минимальный базовый образ)
- Эффективное использование ресурсов

## Лицензия

MIT 

## Расчет оптимального размера буфера канала

### Анализ нагрузки
- RPS (запросов в секунду): ~3,988
- Среднее время обработки задачи: ~14.47ms
- Количество воркеров: 50

### Теоретический расчет
```
Задачи за время обработки = RPS * Время обработки
= 3,988 * 0.01447 ≈ 58 задач

Учитывая пиковые нагрузки (2-3x):
= 58 * 3 ≈ 174 задачи

Учитывая запас на случай задержек (2x):
= 174 * 2 ≈ 348 задач
```

### Практические тесты
| Размер буфера | RPS | Процент ошибок | p95 latency | Теоретический расчет |
|---------------|-----|----------------|-------------|----------------------|
| 500 | 3,874 | 12.85% | 69.05ms | 348 (теоретический) |
| 1000 | 3,932 | 12.34% | 63.65ms | - |
| 5000 | 3,988 | 10.56% | 58.06ms | - |
| 10000 | 3,877 | 11.77% | 66.21ms | - |

### Анализ расхождения теории и практики

1. **Теоретический расчет (348) vs Практика (5000)**:
   - Теория: 348 задач достаточно
   - Практика: 5000 дает лучшие результаты
   - Расхождение: ~14x

2. **Причины расхождения**:
   - **Нелинейность нагрузки**:
     - Теория предполагает равномерное распределение
     - Практика показывает пиковые нагрузки
     - Необходим больший запас прочности

   - **Задержки в MongoDB**:
     - Теория не учитывает задержки БД
     - Практика показывает влияние на производительность
     - Требуется буфер для компенсации

   - **Накладные расходы**:
     - Управление горутинами
     - Контекстные переключения
     - Сборка мусора

   - **Системные ограничения**:
     - CPU scheduling
     - Memory pressure
     - Network latency

3. **Оптимальный размер буфера**:
   - Теоретический минимум: 348
   - Практический оптимум: 5000
   - Рекомендуемый диапазон: 3000-7000

### Объяснение выбора размера 5000

1. **Слишком маленький буфер (500)**:
   - Частые блокировки при записи
   - Высокий процент ошибок (12.85%)
   - Потеря производительности

2. **Оптимальный буфер (5000)**:
   - Достаточно места для пиковых нагрузок
   - Эффективное использование памяти
   - Лучший баланс производительности и надежности
   - Минимальный процент ошибок (10.56%)

3. **Слишком большой буфер (10000)**:
   - Больше памяти в использовании
   - Возможные задержки из-за длинной очереди
   - Незначительное улучшение по сравнению с 5000

### Факторы, влияющие на размер буфера
- Количество воркеров (50)
- Скорость обработки MongoDB
- Пиковые нагрузки
- Доступная память
- Требования к задержкам

### Формула расчета с практическими коэффициентами
```
Оптимальный размер буфера = 
  (Средний RPS * Среднее время обработки) * 
  (Коэффициент пиковой нагрузки) * 
  (Коэффициент запаса) *
  (Практический коэффициент) ≈ 5000

Где:
- Средний RPS = 3,988
- Среднее время обработки = 0.01447
- Коэффициент пиковой нагрузки = 3
- Коэффициент запаса = 2
- Практический коэффициент = 12 (эмпирически подобран)
```

### Возможные улучшения
1. Динамическая настройка размера буфера
2. Мониторинг заполнения очереди
3. Автоматическая подстройка под нагрузку
4. Учет реальных метрик в расчете